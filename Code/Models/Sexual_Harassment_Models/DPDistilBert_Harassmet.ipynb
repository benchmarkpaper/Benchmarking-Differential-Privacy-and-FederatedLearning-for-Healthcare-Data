{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DPDistilBert_Harassmet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFcT_ttb3cxp",
        "outputId": "0faa633a-dd8f-4465-a2a5-10b9924a6858"
      },
      "source": [
        "#Install libraries\n",
        "!pip3 install transformers\n",
        "!pip3 install torch"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbgrlNreFh1t",
        "outputId": "b9295027-aaa1-454e-971c-325373efba12"
      },
      "source": [
        "!pip install opacus"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opacus in /usr/local/lib/python3.7/dist-packages (0.13.0)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.7/dist-packages (from opacus) (0.9.1+cu101)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.7/dist-packages (from opacus) (2.25.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.8.1+cu101)\n",
            "Requirement already satisfied: tqdm>=4.40 in /usr/local/lib/python3.7/dist-packages (from opacus) (4.41.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.9->opacus) (7.1.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->opacus) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI_AcZbA20Ll"
      },
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Use GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGLPc0Pw4a_W"
      },
      "source": [
        "# Setting up GPU\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "1oor0Cj2hQyz",
        "outputId": "8951401f-97fc-4cf7-edd4-2b5fbf7de574"
      },
      "source": [
        "df = pd.read_csv('Harassment_Cleaned_tweets.csv')\n",
        "df.head()"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Key Word</th>\n",
              "      <th>Username</th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Datetime</th>\n",
              "      <th>Favorite_count</th>\n",
              "      <th>Geo</th>\n",
              "      <th>Coordinates</th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>Unnamed: 10</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>704</td>\n",
              "      <td>ass</td>\n",
              "      <td>DeborahParr</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>11-11-2020 06:56</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>He’d have my phone wedged up his ass sideways.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1915</td>\n",
              "      <td>boobies</td>\n",
              "      <td>MaxZorin85</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>11-11-2020 07:35</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>Yep 100% agree and the same with severine in s...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2856</td>\n",
              "      <td>eat pussy</td>\n",
              "      <td>PRISJ1_</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>11-11-2020 10:36</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>Stop having sex with men that won’t eat your p...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2163</td>\n",
              "      <td>Breast Man</td>\n",
              "      <td>Teresamckenzy1</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>10-11-2020 20:52</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>When you see a sad man, just give him breast t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2852</td>\n",
              "      <td>eat pussy</td>\n",
              "      <td>sj__vazquez</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>11-11-2020 10:42</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>We can't be together if you don't eat pussy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0    Key Word        Username  ...  Unnamed: 9 Unnamed: 10  Unnamed: 11\n",
              "0         704         ass     DeborahParr  ...         NaN         NaN          NaN\n",
              "1        1915     boobies      MaxZorin85  ...         NaN         NaN          NaN\n",
              "2        2856   eat pussy         PRISJ1_  ...         NaN         NaN          NaN\n",
              "3        2163  Breast Man  Teresamckenzy1  ...         NaN         NaN          NaN\n",
              "4        2852   eat pussy     sj__vazquez  ...         NaN         NaN          NaN\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJYVU02b4DjQ"
      },
      "source": [
        "#Initialization\n",
        "MAX_LEN = 512\n",
        "VIRTUAL_BATCH_SIZE = 32\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "\n",
        "assert VIRTUAL_BATCH_SIZE % TRAIN_BATCH_SIZE == 0 # VIRTUAL_BATCH_SIZE should be divisible by BATCH_SIZE\n",
        "N_ACCUMULATION_STEPS = int(VIRTUAL_BATCH_SIZE / TRAIN_BATCH_SIZE)\n",
        "\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOV8C2S54FXw"
      },
      "source": [
        "#The class is defined to accept the Dataframe as input and generate tokenized output that is used by the DistilBERT model for training.\n",
        "#The tokenizer uses the encode_plus method to perform tokenization and generate the necessary outputs, namely: ids, attention_mask\n",
        "class Triage(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.data.Text[index])\n",
        "        title = \" \".join(title.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.data.Label[index], dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfPQekmI4JCv",
        "outputId": "238fe792-bf14-47de-9729-a1dde45df5fa"
      },
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state=200)\n",
        "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (3604, 13)\n",
            "TRAIN Dataset: (2883, 13)\n",
            "TEST Dataset: (721, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTGRVAzeF99Q",
        "outputId": "513a54a8-375f-4ea6-ad22-b5294728a81a"
      },
      "source": [
        "SAMPLE_RATE = TRAIN_BATCH_SIZE / len(training_set)\n",
        "LOGGING_INTERVAL = 100 # once every how many steps we run evaluation cycle and report metrics\n",
        "EPSILON = 0.5\n",
        "DELTA = 1 / len(training_set) # Parameter for privacy accounting. Probability of not achieving privacy guarantees\n",
        "DELTA"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.000346860908775581"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DIl0v0P4Lss"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
        "\n",
        "SAMPLE_RATE = TRAIN_BATCH_SIZE / len(training_set)\n",
        "\n",
        "#train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "#                'shuffle': True,\n",
        "#                'num_workers': 0\n",
        "#                }\n",
        "\n",
        "#test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "#                'shuffle': True,\n",
        "#                'num_workers': 0\n",
        "#                }\n",
        "\n",
        "train_sampler=UniformWithReplacementSampler(\n",
        "    num_samples=len(training_set),\n",
        "    sample_rate=SAMPLE_RATE,\n",
        ")\n",
        "\n",
        "test_sampler = SequentialSampler(testing_set)\n",
        "\n",
        "training_loader = DataLoader(training_set, batch_sampler=train_sampler)\n",
        "testing_loader = DataLoader(testing_set, sampler=test_sampler, batch_size=TRAIN_BATCH_SIZE//2)"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0zPlrdH4N1a"
      },
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class DistillBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistillBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 4)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPw39lMQ4P0l",
        "outputId": "b62a7b61-f68d-43f8-9906-e91e6a185f66"
      },
      "source": [
        "model = DistillBERTClass()\n",
        "model.to(device)"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistillBERTClass(\n",
              "  (l1): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1pTmjck4R4J"
      },
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_ple3lZ4hx-"
      },
      "source": [
        "# Function to calcuate the accuracy of the model\n",
        "\n",
        "def calcuate_accuracy(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNg2P4f3GQt6"
      },
      "source": [
        ""
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1aCpDKRGdyU",
        "outputId": "e729a136-b3e0-4163-88da-ae318811e65f"
      },
      "source": [
        "trainable_layers = [model.pre_classifier, model.classifier]\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "\n",
        "for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "        total_params += p.numel()\n",
        "\n",
        "for layer in trainable_layers:\n",
        "    for p in layer.parameters():\n",
        "        p.requires_grad = True\n",
        "        trainable_params += p.numel()\n",
        "\n",
        "print(f\"Total parameters count: {total_params}\") # ~66M\n",
        "print(f\"Trainable parameters count: {trainable_params}\") # ~0.5M"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total parameters count: 66956548\n",
            "Trainable parameters count: 593668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G77DSzFNGO3T",
        "outputId": "7a082e19-917d-41dd-f642-76048dfd4d7f"
      },
      "source": [
        "from opacus import PrivacyEngine\n",
        "\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "privacy_engine = PrivacyEngine(\n",
        "    module=model,\n",
        "    sample_rate=SAMPLE_RATE * N_ACCUMULATION_STEPS,\n",
        "    target_delta = DELTA,\n",
        "    target_epsilon = EPSILON, \n",
        "    epochs = EPOCHS,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")\n",
        "privacy_engine.attach(optimizer)"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyji3VKOXSHO"
      },
      "source": [
        "#Testing the trained model\n",
        "\n",
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            #token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask)\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "            \n",
        "            if _% 1000==0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                accu_step = (n_correct*100)/nb_tr_examples\n",
        "                print(f\"Validation Loss per 5000 steps: {loss_step}\")\n",
        "                print(f\"Validation Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "            \n",
        "        \n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "    \n",
        "    return epoch_accu, epoch_loss\n"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ_b5lp44k83"
      },
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch, training_loader, testing_loader):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    losses=[]\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _% 5000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        \n",
        "        if (_ + 1) % 1000 == 0 or _ == len(training_loader) - 1:\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            optimizer.virtual_step()\n",
        "\n",
        "        if _ > 0 and _ % 1000 == 0:\n",
        "              train_loss = np.mean(losses)\n",
        "              eps, alpha = optimizer.privacy_engine.get_privacy_spent(DELTA)\n",
        "\n",
        "              eval_accuracy,eval_loss = valid(model, testing_loader)\n",
        "\n",
        "              print(\n",
        "                  f\"Epoch: {epoch} | \"\n",
        "                  f\"Step: {_} | \"\n",
        "                  f\"Train loss: {train_loss:.3f} | \"\n",
        "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
        "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
        "                  f\"ɛ: {eps:.2f} (α: {alpha})\"\n",
        "              )\n",
        "\n",
        "        \n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return "
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNp2PYPpXFAF",
        "outputId": "40d4c780-d7b5-4c3b-851d-8aa9a6828167"
      },
      "source": [
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch, training_loader, testing_loader)"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.3436614274978638\n",
            "Training Accuracy per 5000 steps: 37.5\n",
            "The Total Accuracy for Epoch 0: 42.30245231607629\n",
            "Training Loss Epoch: 1.3476680159568786\n",
            "Training Accuracy Epoch: 42.30245231607629\n",
            "Training Loss per 5000 steps: 1.3634719848632812\n",
            "Training Accuracy per 5000 steps: 28.571428571428573\n",
            "The Total Accuracy for Epoch 1: 42.76729559748428\n",
            "Training Loss Epoch: 1.3495464076598485\n",
            "Training Accuracy Epoch: 42.76729559748428\n",
            "Training Loss per 5000 steps: 1.3572901487350464\n",
            "Training Accuracy per 5000 steps: 28.571428571428573\n",
            "The Total Accuracy for Epoch 2: 43.78247315552477\n",
            "Training Loss Epoch: 1.3488687362935807\n",
            "Training Accuracy Epoch: 43.78247315552477\n",
            "Training Loss per 5000 steps: 1.369183897972107\n",
            "Training Accuracy per 5000 steps: 33.333333333333336\n",
            "The Total Accuracy for Epoch 3: 42.73235499650594\n",
            "Training Loss Epoch: 1.3488933854632907\n",
            "Training Accuracy Epoch: 42.73235499650594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqS17KWOXE8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b681eeac-70be-4ae9-cad8-de7dfbdf265a"
      },
      "source": [
        "acc, loss = valid(model, testing_loader)\n",
        "print(\"Accuracy on test data = %0.2f%%\" % acc)"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "1it [00:00,  6.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "2it [00:00,  6.91it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss per 5000 steps: 1.338810920715332\n",
            "Validation Accuracy per 5000 steps: 50.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "3it [00:00,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "4it [00:00,  7.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "5it [00:00,  7.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "6it [00:00,  6.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "7it [00:00,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "8it [00:01,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "9it [00:01,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "10it [00:01,  6.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "11it [00:01,  6.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "12it [00:01,  7.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "13it [00:01,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "14it [00:01,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "15it [00:02,  7.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "16it [00:02,  7.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "17it [00:02,  7.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "18it [00:02,  7.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "19it [00:02,  7.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "20it [00:02,  7.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "21it [00:02,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "22it [00:03,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "23it [00:03,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "24it [00:03,  7.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "25it [00:03,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "26it [00:03,  7.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "27it [00:03,  7.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "28it [00:03,  7.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "29it [00:04,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "30it [00:04,  7.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "31it [00:04,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "32it [00:04,  7.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "33it [00:04,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "34it [00:04,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "35it [00:04,  6.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "36it [00:05,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "37it [00:05,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "38it [00:05,  7.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "39it [00:05,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "40it [00:05,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "41it [00:05,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "42it [00:05,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "43it [00:06,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "44it [00:06,  6.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "45it [00:06,  6.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "46it [00:06,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "47it [00:06,  7.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "48it [00:06,  7.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "49it [00:06,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "50it [00:07,  7.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "51it [00:07,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "52it [00:07,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "53it [00:07,  6.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "54it [00:07,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "55it [00:07,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "56it [00:07,  6.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "57it [00:08,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "58it [00:08,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "59it [00:08,  7.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "60it [00:08,  6.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "61it [00:08,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "62it [00:08,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "63it [00:08,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "64it [00:09,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "65it [00:09,  6.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "66it [00:09,  6.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "67it [00:09,  6.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "68it [00:09,  6.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "69it [00:09,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "70it [00:09,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "71it [00:10,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "72it [00:10,  7.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "73it [00:10,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "74it [00:10,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "75it [00:10,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "76it [00:10,  7.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "77it [00:10,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "78it [00:11,  6.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "79it [00:11,  6.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "80it [00:11,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "81it [00:11,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "82it [00:11,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "83it [00:11,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "84it [00:11,  7.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "85it [00:12,  6.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "86it [00:12,  6.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "87it [00:12,  6.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "88it [00:12,  6.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "89it [00:12,  6.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "90it [00:12,  6.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "91it [00:12,  6.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "92it [00:13,  6.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "93it [00:13,  6.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "94it [00:13,  7.01it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "95it [00:13,  7.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "96it [00:13,  7.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "97it [00:13,  7.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "98it [00:13,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "99it [00:14,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100it [00:14,  7.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "101it [00:14,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "102it [00:14,  7.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "103it [00:14,  7.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "104it [00:14,  7.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "105it [00:14,  7.16it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "106it [00:15,  7.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "107it [00:15,  7.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "108it [00:15,  7.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "109it [00:15,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "110it [00:15,  7.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "111it [00:15,  7.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "112it [00:15,  7.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "113it [00:16,  7.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "114it [00:16,  7.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "115it [00:16,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "116it [00:16,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "117it [00:16,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "118it [00:16,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "119it [00:16,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "120it [00:17,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "121it [00:17,  6.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "122it [00:17,  6.90it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "123it [00:17,  6.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "124it [00:17,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "125it [00:17,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "126it [00:17,  7.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "127it [00:18,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "128it [00:18,  7.04it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "129it [00:18,  7.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "130it [00:18,  7.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "131it [00:18,  7.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "132it [00:18,  7.06it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "133it [00:18,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "134it [00:19,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "135it [00:19,  7.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "136it [00:19,  7.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "137it [00:19,  7.07it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "138it [00:19,  7.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "139it [00:19,  6.92it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "140it [00:19,  6.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "141it [00:20,  6.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "142it [00:20,  6.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "143it [00:20,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "144it [00:20,  6.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "145it [00:20,  6.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "146it [00:20,  6.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "147it [00:20,  6.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "148it [00:21,  7.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "149it [00:21,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "150it [00:21,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "151it [00:21,  6.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "152it [00:21,  6.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "153it [00:21,  6.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "154it [00:21,  6.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "155it [00:22,  6.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "156it [00:22,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "157it [00:22,  7.02it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "158it [00:22,  6.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "159it [00:22,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "160it [00:22,  6.99it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "161it [00:22,  7.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "162it [00:23,  7.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "163it [00:23,  7.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "164it [00:23,  7.05it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "165it [00:23,  6.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "166it [00:23,  6.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "167it [00:23,  6.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "168it [00:23,  6.96it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "169it [00:24,  6.98it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "170it [00:24,  6.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "171it [00:24,  6.93it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "172it [00:24,  6.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "173it [00:24,  6.89it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "174it [00:24,  6.88it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "175it [00:24,  6.91it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "176it [00:25,  7.00it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "177it [00:25,  7.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "178it [00:25,  6.97it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "179it [00:25,  6.95it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "181it [00:25,  7.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss Epoch: 1.346121278915616\n",
            "Validation Accuracy Epoch: 54.36893203883495\n",
            "Accuracy on test data = 54.37%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}