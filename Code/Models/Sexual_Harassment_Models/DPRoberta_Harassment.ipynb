{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DPRoberta_Harassment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPJebXhqhB7q",
        "outputId": "875dc447-e97f-4ce5-9ceb-2373c15443b4"
      },
      "source": [
        "!pip install transformers==3.0.2"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.95)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.25.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (8.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgJzFEMYpce-",
        "outputId": "a9c642d7-b503-48c9-e6af-fbd2ebea9513"
      },
      "source": [
        "!pip install opacus"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opacus in /usr/local/lib/python3.7/dist-packages (0.13.0)\n",
            "Requirement already satisfied: tqdm>=4.40 in /usr/local/lib/python3.7/dist-packages (from opacus) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.8.1+cu101)\n",
            "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.7/dist-packages (from opacus) (2.25.1)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.4.1)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.7/dist-packages (from opacus) (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->opacus) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->opacus) (2020.12.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.9->opacus) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ6UMKaGhMVK"
      },
      "source": [
        "# Importing the libraries needed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SbOk2ZjhOk-"
      },
      "source": [
        "# Setting up GPU\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "1oor0Cj2hQyz",
        "outputId": "5bc1e33e-c0d3-400d-bc07-7f6613a7d7fc"
      },
      "source": [
        "df = pd.read_csv('Harassment_Cleaned_tweets.csv')\n",
        "df.head()"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Key Word</th>\n",
              "      <th>Username</th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Datetime</th>\n",
              "      <th>Favorite_count</th>\n",
              "      <th>Geo</th>\n",
              "      <th>Coordinates</th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>Unnamed: 10</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>704</td>\n",
              "      <td>ass</td>\n",
              "      <td>DeborahParr</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>11-11-2020 06:56</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>Heâ€™d have my phone wedged up his ass sideways.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1915</td>\n",
              "      <td>boobies</td>\n",
              "      <td>MaxZorin85</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>11-11-2020 07:35</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>Yep 100% agree and the same with severine in s...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2856</td>\n",
              "      <td>eat pussy</td>\n",
              "      <td>PRISJ1_</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>11-11-2020 10:36</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>Stop having sex with men that wonâ€™t eat your p...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2163</td>\n",
              "      <td>Breast Man</td>\n",
              "      <td>Teresamckenzy1</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>10-11-2020 20:52</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>When you see a sad man, just give him breast t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2852</td>\n",
              "      <td>eat pussy</td>\n",
              "      <td>sj__vazquez</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>11-11-2020 10:42</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>We can't be together if you don't eat pussy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0    Key Word        Username  ...  Unnamed: 9 Unnamed: 10  Unnamed: 11\n",
              "0         704         ass     DeborahParr  ...         NaN         NaN          NaN\n",
              "1        1915     boobies      MaxZorin85  ...         NaN         NaN          NaN\n",
              "2        2856   eat pussy         PRISJ1_  ...         NaN         NaN          NaN\n",
              "3        2163  Breast Man  Teresamckenzy1  ...         NaN         NaN          NaN\n",
              "4        2852   eat pussy     sj__vazquez  ...         NaN         NaN          NaN\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "B9BHdSsIhVuN",
        "outputId": "d2385ec7-8989-49e6-c6d6-38dfe2ec5744"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Favorite_count</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3604.000000</td>\n",
              "      <td>3.604000e+03</td>\n",
              "      <td>3604.000000</td>\n",
              "      <td>3604.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1801.632908</td>\n",
              "      <td>1.329972e+18</td>\n",
              "      <td>1.429245</td>\n",
              "      <td>0.547447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1040.729184</td>\n",
              "      <td>3.991290e+15</td>\n",
              "      <td>10.752237</td>\n",
              "      <td>0.497813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.320000e+18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>900.750000</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1801.500000</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2702.250000</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3604.000000</td>\n",
              "      <td>1.350000e+18</td>\n",
              "      <td>396.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0       User_ID  Favorite_count        Label\n",
              "count  3604.000000  3.604000e+03     3604.000000  3604.000000\n",
              "mean   1801.632908  1.329972e+18        1.429245     0.547447\n",
              "std    1040.729184  3.991290e+15       10.752237     0.497813\n",
              "min       0.000000  1.320000e+18        0.000000     0.000000\n",
              "25%     900.750000  1.330000e+18        0.000000     0.000000\n",
              "50%    1801.500000  1.330000e+18        0.000000     1.000000\n",
              "75%    2702.250000  1.330000e+18        1.000000     1.000000\n",
              "max    3604.000000  1.350000e+18      396.000000     1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I09eJvCVhcJn"
      },
      "source": [
        "#Preparing Dataset and Dataloader\n",
        "\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "VIRTUAL_BATCH_SIZE = 32\n",
        "assert VIRTUAL_BATCH_SIZE % TRAIN_BATCH_SIZE == 0 # VIRTUAL_BATCH_SIZE should be divisible by BATCH_SIZE\n",
        "N_ACCUMULATION_STEPS = int(VIRTUAL_BATCH_SIZE / TRAIN_BATCH_SIZE)\n",
        "# EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8lkbc__hoUV"
      },
      "source": [
        "class TweetData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.Text\n",
        "        self.targets = self.data.Label\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CVkBjDAhygJ",
        "outputId": "73ca656d-5b0a-4ead-a8ca-f3f1136751f7"
      },
      "source": [
        "train_size = 0.8\n",
        "train_data=df.sample(frac=train_size,random_state=200)\n",
        "test_data=df.drop(train_data.index).reset_index(drop=True)\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
        "\n",
        "training_set = TweetData(train_data, tokenizer, MAX_LEN)\n",
        "testing_set = TweetData(test_data, tokenizer, MAX_LEN)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (3604, 13)\n",
            "TRAIN Dataset: (2883, 13)\n",
            "TEST Dataset: (721, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymYx7dSipK6l"
      },
      "source": [
        "LOGGING_INTERVAL = 100 # once every how many steps we run evaluation cycle and report metrics\n",
        "EPSILON = 0.5\n",
        "DELTA = 1 / len(training_set) # Parameter for privacy accounting. Probability of not achieving privacy guarantees"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVWj4V2Jh7ZO"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
        "\n",
        "SAMPLE_RATE = TRAIN_BATCH_SIZE / len(training_set)\n",
        "\n",
        "#train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "#                'shuffle': True,\n",
        "#                'num_workers': 0\n",
        "#                }\n",
        "\n",
        "#test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "#                'shuffle': True,\n",
        "#                'num_workers': 0\n",
        "#                }\n",
        "\n",
        "train_sampler=UniformWithReplacementSampler(\n",
        "    num_samples=len(training_set),\n",
        "    sample_rate=SAMPLE_RATE,\n",
        ")\n",
        "\n",
        "test_sampler = SequentialSampler(testing_set)\n",
        "\n",
        "training_loader = DataLoader(training_set, batch_sampler=train_sampler)\n",
        "testing_loader = DataLoader(testing_set, sampler=test_sampler, batch_size=VALID_BATCH_SIZE)"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uotZ8qrQpsid"
      },
      "source": [
        ""
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eENOAhKgpsgs"
      },
      "source": [
        ""
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcqCzmTOh9N2"
      },
      "source": [
        "#Base Roberta model\n",
        "class RobertaClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RobertaClass, self).__init__()\n",
        "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 5)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6srQnPth-4m",
        "outputId": "40630735-f41f-408b-8f53-6992075cfcc5"
      },
      "source": [
        "model = RobertaClass()\n",
        "model.to(device)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaClass(\n",
              "  (l1): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8bn-3RgrUCO",
        "outputId": "74bc8e0c-78c3-47d7-b117-2916773ef47c"
      },
      "source": [
        "trainable_layers = [model.pre_classifier, model.classifier]\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "\n",
        "for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "        total_params += p.numel()\n",
        "\n",
        "for layer in trainable_layers:\n",
        "    for p in layer.parameters():\n",
        "        p.requires_grad = True\n",
        "        trainable_params += p.numel()\n",
        "\n",
        "print(f\"Total parameters count: {total_params}\") # ~125M\n",
        "print(f\"Trainable parameters count: {trainable_params}\") # ~0.5M"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total parameters count: 125240069\n",
            "Trainable parameters count: 594437\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1WJwqgeiA1b"
      },
      "source": [
        "#Finetuning Roberta model\n",
        "\n",
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvU-ZlDNiJ3l"
      },
      "source": [
        "def calcuate_accuracy(preds, targets):\n",
        "    n_correct = (preds==targets).sum().item()\n",
        "    return n_correct"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsTFEV6brKmk"
      },
      "source": [
        "EPOCHS = 1"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPpVkFPWxMcy"
      },
      "source": [
        ""
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6L7r0M7pfwo",
        "outputId": "7a647eab-0cd9-45fe-9f83-c5d88738f29d"
      },
      "source": [
        "from opacus import PrivacyEngine\n",
        "\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "privacy_engine = PrivacyEngine(\n",
        "    module=model,\n",
        "    sample_rate=SAMPLE_RATE * N_ACCUMULATION_STEPS,\n",
        "    target_delta = DELTA,\n",
        "    target_epsilon = EPSILON, \n",
        "    epochs = EPOCHS,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")\n",
        "privacy_engine.attach(optimizer)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkLN2DUJiRLp"
      },
      "source": [
        "#Testing the trained model\n",
        "\n",
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.long)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples+=targets.size(0)\n",
        "            \n",
        "            if _%5000==0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                accu_step = (n_correct*100)/nb_tr_examples\n",
        "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
        "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
        "\n",
        "            \n",
        "        \n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "    \n",
        "    return epoch_accu, epoch_loss\n"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9bBf0wniKmU"
      },
      "source": [
        "# Defining the training function on the 80% of the dataset\n",
        "\n",
        "def train(epoch, training_loader, testing_loader):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    losses = []\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "        \n",
        "        if _%2000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "        # # When using GPU\n",
        "        if (_ + 1) % 2000 == 0 or _ == len(training_loader) - 1:\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            optimizer.virtual_step()\n",
        "\n",
        "        if _ > 0 and _ % 2000 == 0:\n",
        "              train_loss = np.mean(losses)\n",
        "              eps, alpha = optimizer.privacy_engine.get_privacy_spent(DELTA)\n",
        "\n",
        "              eval_accuracy,eval_loss = valid(model, testing_loader)\n",
        "\n",
        "              print(\n",
        "                  f\"Epoch: {epoch} | \"\n",
        "                  f\"Step: {_} | \"\n",
        "                  f\"Train loss: {train_loss:.3f} | \"\n",
        "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
        "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
        "                  f\"É›: {eps:.2f} (Î±: {alpha})\"\n",
        "              )\n",
        "\n",
        "        \n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return "
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6uzrRotiN7x",
        "outputId": "4b10f3a0-27cd-42db-b013-2ef4b0ce9586"
      },
      "source": [
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch, training_loader, testing_loader)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "\n",
            "\n",
            "1it [00:00,  7.58it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss per 5000 steps: 1.5928491353988647\n",
            "Training Accuracy per 5000 steps: 40.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "2it [00:00,  6.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "3it [00:00,  6.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "4it [00:00,  6.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "6it [00:00,  7.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "8it [00:00,  8.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "9it [00:01,  8.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "10it [00:01,  7.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "11it [00:01,  7.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "13it [00:01,  8.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "14it [00:01,  7.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "15it [00:01,  6.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "16it [00:02,  7.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "17it [00:02,  7.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "18it [00:02,  7.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "19it [00:02,  7.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "20it [00:02,  6.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "22it [00:02,  7.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "23it [00:02,  7.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "24it [00:03,  7.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "26it [00:03,  8.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "27it [00:03,  7.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "29it [00:03,  7.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "31it [00:03,  8.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "33it [00:04,  8.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "34it [00:04,  6.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "35it [00:04,  7.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "36it [00:04,  7.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "38it [00:04,  7.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "39it [00:04,  7.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "41it [00:05,  8.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "42it [00:05,  8.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "43it [00:05,  8.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "44it [00:05,  8.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "45it [00:05,  7.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "46it [00:05,  7.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "47it [00:05,  6.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "48it [00:06,  7.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "49it [00:06,  6.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "50it [00:06,  6.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "51it [00:06,  6.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "52it [00:06,  7.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "53it [00:06,  6.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "55it [00:06,  7.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "56it [00:07,  7.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "57it [00:07,  7.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "58it [00:07,  7.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "59it [00:07,  8.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "60it [00:07,  7.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "61it [00:07,  8.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "62it [00:07,  8.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "63it [00:08,  7.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "64it [00:08,  7.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "65it [00:08,  7.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "66it [00:08,  7.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "68it [00:08,  8.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "70it [00:08,  9.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "71it [00:08,  8.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "73it [00:09,  8.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "74it [00:09,  7.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "76it [00:09,  8.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "77it [00:09,  7.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "78it [00:09,  7.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "79it [00:09,  6.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "81it [00:10,  7.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "82it [00:10,  7.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "84it [00:10,  8.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "86it [00:10,  8.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "87it [00:10,  8.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "88it [00:10,  7.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "89it [00:11,  7.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "90it [00:11,  6.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "92it [00:11,  7.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "93it [00:11,  7.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "94it [00:11,  7.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "96it [00:11,  7.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "97it [00:12,  6.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "98it [00:12,  5.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "99it [00:12,  6.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "101it [00:12,  7.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "102it [00:12,  6.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "103it [00:13,  6.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "104it [00:13,  6.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "106it [00:13,  7.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "108it [00:13,  7.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "109it [00:13,  7.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "110it [00:13,  7.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "111it [00:14,  7.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "112it [00:14,  7.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "113it [00:14,  6.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "114it [00:14,  6.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "115it [00:14,  7.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "116it [00:14,  7.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "117it [00:14,  7.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "119it [00:15,  8.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "120it [00:15,  7.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "121it [00:15,  7.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "122it [00:15,  5.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "124it [00:15,  6.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "125it [00:16,  6.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "127it [00:16,  6.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "128it [00:16,  6.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "130it [00:16,  7.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "132it [00:16,  8.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "133it [00:16,  8.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "135it [00:17,  8.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "136it [00:17,  8.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "137it [00:17,  7.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "139it [00:17,  8.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "141it [00:17,  8.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "142it [00:18,  6.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "144it [00:18,  8.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "145it [00:18,  8.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "146it [00:18,  7.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "147it [00:18,  8.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "149it [00:18,  8.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "150it [00:19,  8.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "152it [00:19,  9.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "153it [00:19,  8.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "154it [00:19,  9.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "155it [00:19,  9.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "156it [00:19,  8.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "158it [00:19,  8.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "159it [00:20,  7.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "160it [00:20,  8.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "161it [00:20,  7.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "162it [00:20,  7.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "163it [00:20,  7.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "164it [00:20,  7.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "165it [00:20,  7.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "166it [00:20,  7.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "167it [00:21,  7.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "168it [00:21,  7.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "169it [00:21,  7.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "170it [00:21,  6.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "171it [00:21,  6.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "172it [00:21,  6.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "174it [00:22,  7.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "175it [00:22,  7.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "177it [00:22,  8.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "178it [00:22,  6.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "180it [00:22,  7.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "181it [00:22,  7.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "182it [00:23,  7.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "183it [00:23,  7.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "184it [00:23,  7.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "185it [00:23,  7.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "186it [00:23,  7.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "187it [00:23,  7.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "188it [00:23,  7.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "189it [00:24,  7.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "190it [00:24,  6.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "191it [00:24,  6.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "192it [00:24,  6.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "193it [00:24,  7.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "194it [00:24,  7.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "195it [00:24,  6.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "196it [00:25,  6.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "197it [00:25,  7.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "199it [00:25,  7.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "200it [00:25,  7.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "201it [00:25,  7.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "202it [00:25,  7.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "203it [00:26,  6.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "204it [00:26,  6.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "205it [00:26,  7.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "206it [00:26,  7.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "208it [00:26,  7.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "210it [00:26,  8.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "211it [00:27,  7.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "212it [00:27,  7.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "213it [00:27,  6.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "215it [00:27,  7.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "217it [00:27,  8.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "218it [00:27,  7.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "219it [00:28,  7.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "221it [00:28,  8.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "222it [00:28,  8.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "223it [00:28,  7.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "224it [00:28,  7.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "225it [00:28,  6.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "226it [00:28,  6.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "227it [00:29,  6.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "228it [00:29,  7.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "229it [00:29,  6.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "230it [00:29,  6.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "232it [00:29,  6.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "233it [00:29,  6.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "234it [00:30,  6.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "235it [00:30,  6.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "237it [00:30,  6.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "239it [00:30,  7.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "240it [00:31,  6.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "241it [00:31,  6.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "242it [00:31,  6.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "244it [00:31,  6.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "245it [00:31,  6.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "246it [00:31,  6.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "248it [00:31,  8.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "250it [00:32,  7.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "251it [00:32,  7.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "253it [00:32,  7.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "254it [00:32,  8.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "255it [00:32,  7.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "256it [00:33,  7.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "257it [00:33,  7.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "258it [00:33,  7.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "259it [00:33,  7.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "261it [00:33,  7.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "262it [00:33,  7.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "263it [00:34,  6.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "265it [00:34,  6.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "267it [00:34,  7.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "268it [00:34,  7.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "270it [00:34,  7.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "271it [00:34,  8.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "272it [00:35,  8.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "274it [00:35,  8.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "275it [00:35,  8.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "276it [00:35,  7.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "277it [00:35,  7.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "278it [00:35,  6.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "279it [00:36,  6.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "280it [00:36,  6.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "281it [00:36,  6.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "283it [00:36,  7.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "284it [00:36,  6.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "285it [00:36,  6.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "286it [00:37,  6.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "288it [00:37,  7.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "289it [00:37,  7.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "290it [00:37,  7.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "291it [00:37,  7.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "293it [00:37,  8.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "294it [00:37,  8.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "295it [00:38,  7.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "296it [00:38,  7.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "297it [00:38,  6.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "298it [00:38,  6.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "300it [00:38,  7.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "301it [00:39,  6.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "302it [00:39,  6.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "303it [00:39,  6.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "305it [00:39,  7.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "306it [00:39,  7.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "307it [00:39,  7.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "309it [00:39,  8.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "311it [00:40,  9.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "312it [00:40,  8.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "313it [00:40,  8.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "314it [00:40,  7.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "315it [00:40,  7.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "316it [00:40,  7.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "317it [00:40,  7.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "318it [00:41,  7.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "319it [00:41,  8.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "321it [00:41,  8.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "322it [00:41,  6.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "323it [00:41,  7.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "324it [00:41,  6.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "325it [00:42,  7.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "326it [00:42,  7.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "327it [00:42,  7.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "329it [00:42,  8.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "330it [00:42,  8.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "332it [00:42,  9.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "333it [00:42,  9.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "334it [00:43,  8.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "335it [00:43,  8.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "337it [00:43,  8.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "338it [00:43,  8.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "339it [00:43,  8.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "340it [00:43,  8.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "341it [00:43,  7.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "342it [00:44,  6.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "343it [00:44,  7.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "344it [00:44,  6.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "345it [00:44,  7.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "347it [00:44,  7.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "348it [00:44,  7.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "349it [00:45,  6.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "350it [00:45,  6.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "352it [00:45,  6.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "353it [00:45,  5.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "354it [00:45,  6.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "355it [00:45,  6.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "356it [00:46,  6.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "357it [00:46,  6.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "358it [00:46,  6.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "359it [00:46,  6.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "360it [00:46,  7.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Total Accuracy for Epoch 0: 33.57868909919383\n",
            "Training Loss Epoch: 1.5678919629918204\n",
            "Training Accuracy Epoch: 33.57868909919383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlQFNO17iw7h",
        "outputId": "e9da8de5-fe30-46ef-c32f-f2ff404917a1"
      },
      "source": [
        "acc, loss = valid(model, testing_loader)\n",
        "print(\"Accuracy on test data = %0.2f%%\" % acc)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "2it [00:00, 17.56it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss per 100 steps: 1.5637696981430054\n",
            "Validation Accuracy per 100 steps: 50.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "4it [00:00, 17.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "6it [00:00, 16.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "8it [00:00, 16.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "10it [00:00, 16.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "12it [00:00, 16.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "14it [00:00, 16.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "16it [00:00, 16.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "18it [00:01, 16.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "20it [00:01, 16.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "22it [00:01, 16.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "24it [00:01, 16.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "26it [00:01, 16.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "28it [00:01, 16.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "30it [00:01, 16.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "32it [00:01, 16.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "34it [00:02, 16.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "36it [00:02, 16.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "38it [00:02, 16.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "40it [00:02, 16.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "42it [00:02, 16.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "44it [00:02, 16.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "46it [00:02, 16.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "48it [00:02, 16.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "50it [00:03, 16.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "52it [00:03, 16.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "54it [00:03, 16.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "56it [00:03, 16.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "58it [00:03, 16.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "60it [00:03, 16.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "62it [00:03, 16.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "64it [00:03, 16.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "66it [00:04, 16.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "68it [00:04, 16.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "70it [00:04, 16.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "72it [00:04, 16.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "74it [00:04, 16.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "76it [00:04, 16.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "78it [00:04, 16.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "80it [00:04, 16.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "82it [00:05, 16.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "84it [00:05, 16.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "86it [00:05, 16.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "88it [00:05, 16.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "90it [00:05, 16.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "92it [00:05, 16.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "94it [00:05, 16.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "96it [00:05, 16.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "98it [00:05, 16.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "100it [00:06, 16.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "102it [00:06, 16.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "104it [00:06, 16.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "106it [00:06, 16.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "108it [00:06, 16.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "110it [00:06, 16.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "112it [00:06, 16.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "114it [00:06, 16.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "116it [00:07, 16.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "118it [00:07, 16.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "120it [00:07, 16.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "122it [00:07, 16.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "124it [00:07, 16.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "126it [00:07, 16.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "128it [00:07, 16.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "130it [00:07, 16.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "132it [00:08, 16.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "134it [00:08, 16.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "136it [00:08, 16.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "138it [00:08, 16.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "140it [00:08, 16.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "142it [00:08, 16.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "144it [00:08, 16.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "146it [00:08, 16.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "148it [00:09, 16.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "150it [00:09, 16.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "152it [00:09, 16.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "154it [00:09, 16.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "156it [00:09, 16.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "158it [00:09, 16.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "160it [00:09, 16.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "162it [00:09, 16.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "164it [00:10, 16.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "166it [00:10, 16.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "168it [00:10, 16.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "170it [00:10, 16.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "172it [00:10, 16.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "174it [00:10, 16.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "176it [00:10, 16.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "178it [00:10, 16.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "181it [00:11, 16.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Loss Epoch: 1.5645007448301789\n",
            "Validation Accuracy Epoch: 52.56588072122053\n",
            "Accuracy on test data = 52.57%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KjBRxC2izrq",
        "outputId": "0cd67371-ea23-48bd-a4d2-e32938537369"
      },
      "source": [
        "output_model_file = 'pytorch_roberta_sentiment.bin'\n",
        "output_vocab_file = './'\n",
        "\n",
        "model_to_save = model\n",
        "torch.save(model_to_save, output_model_file)\n",
        "tokenizer.save_vocabulary(output_vocab_file)\n"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./vocab.json', './merges.txt')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0vG-iBykSrT"
      },
      "source": [
        ""
      ],
      "execution_count": 225,
      "outputs": []
    }
  ]
}